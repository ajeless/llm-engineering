{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c342de-624a-4c9e-8f0b-9b07f514c7d5",
   "metadata": {},
   "source": [
    "# **Ollama Interaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11665172-792f-4462-9d45-08fae7b7c999",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![ollama](../assets/ollama/ollama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa4188-1b8b-4b11-9a64-3947b3efb056",
   "metadata": {},
   "source": [
    "## Python Requests Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf81329-cd27-4806-992e-35717670e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "current_model = get_random_ollama_model()\n",
    "\n",
    "# Endpoint for generating text\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": current_model,\n",
    "    \"prompt\": user_prompt,Z\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55eed0-7c15-4cb3-93fb-ea866d662eb0",
   "metadata": {},
   "source": [
    "## Ollama Python SDK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b0386-307f-4799-a43a-ae9146921332",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ollama          # Official Python client for a local Ollama daemon\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "current_model = get_random_ollama_model()\n",
    "\n",
    "# Send a single-turn \"chat\" to the chosen model.\n",
    "# The chat API expects a list of messages with roles ('system' | 'user' | 'assistant').\n",
    "# Here we send only one user message. You can add a 'system' message to steer behavior.\n",
    "response: ollama.ChatResponse = ollama.chat(\n",
    "    model=current_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    ")\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "# The generated text is accessible at response.message.content for non-streaming calls.\n",
    "print(response.message.content)\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e34581-b7da-4a9e-93af-363c255ae83c",
   "metadata": {},
   "source": [
    "## OpenAI Python SDK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2f9fd-c430-4559-b5a0-a50d4829a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "current_model = get_random_ollama_model()\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=current_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    ")\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "# The generated text\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72abd8-b165-4e19-964f-3d560b269cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
