{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c342de-624a-4c9e-8f0b-9b07f514c7d5",
   "metadata": {},
   "source": [
    "# **OpenAI Interaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11665172-792f-4462-9d45-08fae7b7c999",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![OpenAI](../assets/openai-logos-2025/OpenAI-logos(new)/SVGs/OpenAI-black-monoblossom.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa4188-1b8b-4b11-9a64-3947b3efb056",
   "metadata": {},
   "source": [
    "## Python Requests Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf81329-cd27-4806-992e-35717670e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5-mini\n",
      "\n",
      "I am ChatGPT, powered by OpenAI’s GPT-4o family of models (developed by OpenAI).\n",
      "\n",
      "Why I was created\n",
      "- To assist people and teams with a broad range of language- and knowledge‑based tasks: answering questions, drafting and editing text, helping with code, summarizing and explaining complex topics, tutoring, brainstorming, and handling multimodal inputs (text + images).  \n",
      "- To provide an easy-to-use general-purpose AI assistant that balances usefulness, coherence, and safety for many everyday and professional workflows.\n",
      "\n",
      "Strengths\n",
      "- Broad competence across many domains (writing, reasoning, coding, explanation, translation, summarization).  \n",
      "- Strong instruction-following and ability to produce well-structured outputs.  \n",
      "- Multimodal capability (can work with text and images) when image input is available.  \n",
      "- Fast responses and flexible conversational behavior (can be concise or detailed).  \n",
      "- Designed with safety mitigations and guardrails to reduce harmful or unsafe outputs.\n",
      "\n",
      "Weaknesses and limits\n",
      "- Knowledge cutoff: I don’t know events or new facts after June 2024, and I have no live web access unless you provide up-to-date content.  \n",
      "- I can hallucinate (state incorrect facts or fabricate citations); outputs should be verified for critical use (legal, medical, safety-critical, financial decisions).  \n",
      "- I have limited persistent memory across sessions unless a memory feature is specifically enabled; I don’t automatically retain personal data.  \n",
      "- May reflect biases present in training data and can be sensitive to prompt wording or adversarial inputs.  \n",
      "- I can’t perform real-world actions, run external programs, or access private systems unless you provide the data or integrate me into a system that does so.\n",
      "\n",
      "When you should choose me over other models\n",
      "- You want an all-purpose, high-quality assistant for writing, brainstorming, tutoring, code help, or complex multi-step reasoning.  \n",
      "- You need a model that handles instructions robustly and can produce polished, well-structured outputs.  \n",
      "- You want multimodal help (e.g., interpret or annotate images alongside text).  \n",
      "- You need a balance of capability and safety without building or maintaining a custom model.\n",
      "\n",
      "When another choice may be better\n",
      "- You need guaranteed up-to-the-minute information or live web access — use systems with real-time data integration or search-enabled agents.  \n",
      "- You require absolute determinism, on-premise deployment, or specific regulatory/privacy guarantees — a privately hosted or specialized model may be preferable.  \n",
      "- For extremely narrow, domain-specific tasks where a fine-tuned specialist model outperforms a generalist.\n",
      "\n",
      "If you’d like, I can compare me more directly to a specific model (e.g., GPT-3.5, a proprietary on-prem model, or another vendor’s model) for your exact use case.\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "import random\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.environ[\"OPENAI_API_KEY\"]}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Endpoint for listing models\n",
    "url = \"https://api.openai.com/v1/models\"\n",
    "response = requests.get(url, headers=headers, timeout=30)\n",
    "\n",
    "openai_models = [openai_model[\"id\"] for openai_model in response.json().get(\"data\", [])]\n",
    "\n",
    "def is_chat_model(model_id: str) -> bool:\n",
    "    \"\"\"Heuristic filter: include chat-capable LLMs; exclude embeddings/moderation/audio.\"\"\"\n",
    "    s = model_id.lower()\n",
    "    if any(bad in s for bad in (\"embedding\", \"embed\", \"moderation\", \"whisper\", \"tts\", \"audio\", \"transcribe\", \"pro\", \"turbo\", \"search\", \"preview\")):\n",
    "        return False\n",
    "    # Common chat families (Responses-friendly): GPT-x and o-series\n",
    "    return s.startswith((\"gpt-\", \"gpt\", \"o\", \"chatgpt\"))\n",
    "\n",
    "chat_models = sorted([m for m in openai_models if is_chat_model(m)])\n",
    "\n",
    "current_model = random.choice(chat_models)\n",
    "\n",
    "# Endpoint for generating text\n",
    "url = \"https://api.openai.com/v1/responses\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": current_model,\n",
    "    \"input\": user_prompt\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "response_json = response.json()\n",
    "\n",
    "def extract_text(response: dict) -> str:\n",
    "    # Prefer the convenience field if present; otherwise collect text from parts\n",
    "    if \"output_text\" in response and response[\"output_text\"]:\n",
    "        return response[\"output_text\"]\n",
    "    parts = []\n",
    "    for item in response.get(\"output\", []):\n",
    "        for c in item.get(\"content\", []):\n",
    "            if isinstance(c, dict) and \"text\" in c:\n",
    "                parts.append(c[\"text\"])\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "print(extract_text(response_json))\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e34581-b7da-4a9e-93af-363c255ae83c",
   "metadata": {},
   "source": [
    "## OpenAI Python SDK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a2f9fd-c430-4559-b5a0-a50d4829a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5-2025-08-07\n",
      "\n",
      "- Model and creator\n",
      "  - I’m ChatGPT, a GPT‑4‑class assistant created by OpenAI.\n",
      "\n",
      "- Why I was created\n",
      "  - To serve as a general-purpose reasoning and writing assistant that helps people understand information, solve problems, create content, and interact with software using natural language (and images where relevant), while aiming to be safe, useful, and broadly accessible.\n",
      "\n",
      "- Strengths\n",
      "  - Strong general reasoning and explanation: can break down complex topics into clear, structured steps.\n",
      "  - Writing and editing: drafting, revising, summarizing, translating, and adapting tone for different audiences.\n",
      "  - Multimodal understanding: can discuss and analyze images alongside text.\n",
      "  - Coding help: explain concepts, outline approaches, and generate or refactor code in many languages.\n",
      "  - Instruction following: stays on task, adapts style and level of detail, and can provide checklists or stepwise guidance.\n",
      "  - Safety and privacy orientation: doesn’t access external data or personal files unless you provide them; designed with guardrails to avoid harmful content.\n",
      "\n",
      "- Weaknesses\n",
      "  - Knowledge cutoff: my training data goes up to October 2024; I don’t have live internet access unless the tool is explicitly enabled in your interface.\n",
      "  - Possible inaccuracies: I can make mistakes or sound confident when uncertain; facts, citations, and numbers should be verified for high-stakes use.\n",
      "  - Not a substitute for professionals: medical, legal, financial, or safety-critical advice should be reviewed by qualified experts.\n",
      "  - Limited tooling: if your interface doesn’t enable browsing, code execution, or plugins, I can’t run code, fetch real-time data, or operate external systems directly.\n",
      "  - Image limits: good at many vision tasks, but not perfect for tiny text, ambiguous images, or specialized domains (e.g., medical imaging).\n",
      "\n",
      "- When it’s preferable to choose me over other models\n",
      "  - You need a versatile generalist that combines solid reasoning with strong writing, explanation, and editing.\n",
      "  - You want to work across text and images in a single conversation (e.g., analyzing a screenshot, diagram, or photo along with instructions).\n",
      "  - You prefer clear, stepwise, safety-conscious guidance and the ability to adapt tone/format quickly (brief bullet points, outlines, or structured plans).\n",
      "  - You need multilingual support or to translate/explain content across languages while preserving meaning and style.\n",
      "  - You want help brainstorming and iterating on creative or technical ideas, then tightening them into polished outputs.\n",
      "  - You value predictable, instruction-following behavior for tasks like documentation, specs, lesson plans, or checklists.\n",
      "\n",
      "If you tell me more about your specific task, I can suggest whether I’m a good fit or if a different, more specialized or real-time model would serve you better.\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "import random\n",
    "\n",
    "# Initialize the SDK client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "all_models = client.models.list()\n",
    "openai_models = [model.id for model in all_models.data]\n",
    "\n",
    "def is_chat_model(model_id: str) -> bool:\n",
    "    \"\"\"Heuristic filter: include chat-capable LLMs; exclude embeddings/moderation/audio.\"\"\"\n",
    "    s = model_id.lower()\n",
    "    if any(bad in s for bad in (\n",
    "        \"embedding\", \"embed\", \"moderation\", \"whisper\",\n",
    "        \"tts\", \"audio\", \"transcribe\", \"pro\", \"turbo\",\n",
    "        \"search\", \"preview\"\n",
    "    )):\n",
    "        return False\n",
    "    return s.startswith((\"gpt-\", \"gpt\", \"o\", \"chatgpt\"))\n",
    "\n",
    "chat_models = sorted([m for m in openai_models if is_chat_model(m)])\n",
    "current_model = random.choice(chat_models)\n",
    "\n",
    "def extract_text(response) -> str:\n",
    "    if getattr(response, \"output_text\", None):\n",
    "        return response.output_text\n",
    "    # fallback: collect pieces\n",
    "    parts = []\n",
    "    for item in response.output or []:\n",
    "        for c in getattr(item, \"content\", []):\n",
    "            if hasattr(c, \"text\"):\n",
    "                parts.append(c.text)\n",
    "    return \"\".join(parts)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=current_model,\n",
    "    input=user_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "# The generated text\n",
    "print(extract_text(response))\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72abd8-b165-4e19-964f-3d560b269cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
