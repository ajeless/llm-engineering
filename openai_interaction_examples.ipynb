{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c342de-624a-4c9e-8f0b-9b07f514c7d5",
   "metadata": {},
   "source": [
    "# **OpenAI Interaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11665172-792f-4462-9d45-08fae7b7c999",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![OpenAI](assets/openai-logos-2025/OpenAI-logos(new)/SVGs/OpenAI-black-monoblossom.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa4188-1b8b-4b11-9a64-3947b3efb056",
   "metadata": {},
   "source": [
    "## Python Requests Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbf81329-cd27-4806-992e-35717670e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o3-2025-04-16\n",
      "\n",
      "Model  \n",
      "• Name: ChatGPT (based on OpenAI’s GPT-4 architecture)  \n",
      "• Creator: OpenAI, an AI research and deployment company  \n",
      "\n",
      "Why the model was created  \n",
      "OpenAI’s mission is to ensure that artificial general intelligence benefits all of humanity. GPT-4 was trained to:  \n",
      "1. Serve as a general-purpose natural-language interface, enabling people to obtain information, write, code, brainstorm, translate, tutor and more through ordinary conversation.  \n",
      "2. Demonstrate and advance the state of large-language-model research, providing insight into alignment, safety and responsible deployment at scale.  \n",
      "3. Offer developers and organizations a flexible tool they can adapt for specialized applications (e.g., customer support, education, creative tools, programming assistants).\n",
      "\n",
      "Core strengths  \n",
      "• Breadth of knowledge: Trained on a large and diverse text corpus, it can address a wide range of topics.  \n",
      "• Reasoning ability: Performs well on many standardized tests and multi-step problem-solving tasks.  \n",
      "• Fluency: Generates coherent, context-aware, multi-paragraph text in many styles and tones.  \n",
      "• Multilingual support: Can understand and produce text in dozens of languages.  \n",
      "• Flexibility: Easily steered with prompts; can summarize, expand, translate, analyze, or mimic styles.  \n",
      "• Safety features: Incorporates alignment and content-filtering work aimed at reducing disallowed or harmful outputs.\n",
      "\n",
      "Key weaknesses  \n",
      "• Knowledge cutoff: No built-in awareness of events, facts or research published after its last training update (2024-06).  \n",
      "• Hallucinations: May produce plausible-sounding but incorrect or unverifiable information if the prompt or domain is sparse, ambiguous or highly specialized.  \n",
      "• Inconsistency: Answers can vary between sessions and sometimes even within a single session.  \n",
      "• Lack of true understanding or perception: Operates purely on textual patterns; has no sensory input, personal experience or consciousness.  \n",
      "• Context window limits: Extremely long documents may exceed its input capacity, leading to truncation or loss of detail.  \n",
      "• Dependence on prompt quality: Results strongly reflect how questions are asked; unclear prompts yield weaker answers.\n",
      "\n",
      "When choosing this model over others  \n",
      "ChatGPT-GPT-4 is generally preferable when you need:  \n",
      "1. Broad, balanced competence across many fields rather than narrow, domain-specific mastery.  \n",
      "2. Complex or creative text generation (e.g., drafting articles, scripts, lesson plans) with attention to tone, structure and style.  \n",
      "3. Multi-step reasoning that benefits from chain-of-thought, structured analysis or code generation.  \n",
      "4. A safety-conscious assistant for public-facing applications, thanks to built-in content safeguards and alignment work.  \n",
      "5. Multilingual dialogue where consistent quality across languages matters.  \n",
      "6. Rapid prototyping: a single model that can be prompted for summarization, Q&A, data extraction, ideation, and more without retraining.\n",
      "\n",
      "Situations where another model might be better  \n",
      "• Highly specialized scientific, legal or medical domains demanding citation-level precision.  \n",
      "• Real-time data retrieval or dynamic knowledge of post-cutoff events.  \n",
      "• On-device, privacy-critical deployments where a smaller open-source model can be run locally.  \n",
      "• Extremely budget-sensitive scenarios that can accept simpler language capability.\n",
      "\n",
      "In short, GPT-4’s main appeal is its versatility and strong general performance with integrated safety features, making it a solid default choice when you need a single conversational AI that “does a bit of everything” at a high level.\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "import random\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "# Endpoint for listing models\n",
    "url = \"https://api.openai.com/v1/models\"\n",
    "response = requests.get(url, headers=headers, timeout=30)\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.environ[\"OPENAI_API_KEY\"]}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "openai_models = [openai_model[\"id\"] for openai_model in response.json().get(\"data\", [])]\n",
    "\n",
    "def is_chat_model(model_id: str) -> bool:\n",
    "    \"\"\"Heuristic filter: include chat-capable LLMs; exclude embeddings/moderation/audio.\"\"\"\n",
    "    s = model_id.lower()\n",
    "    if any(bad in s for bad in (\"embedding\", \"embed\", \"moderation\", \"whisper\", \"tts\", \"audio\", \"transcribe\", \"pro\", \"turbo\", \"search\", \"preview\")):\n",
    "        return False\n",
    "    # Common chat families (Responses-friendly): GPT-x and o-series\n",
    "    return s.startswith((\"gpt-\", \"gpt\", \"o\", \"chatgpt\"))\n",
    "\n",
    "chat_models = sorted([m for m in openai_models if is_chat_model(m)])\n",
    "\n",
    "current_model = random.choice(chat_models)\n",
    "\n",
    "# Endpoint for generating text\n",
    "url = \"https://api.openai.com/v1/responses\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": current_model,\n",
    "    \"input\": user_prompt\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "response_json = response.json()\n",
    "\n",
    "def extract_text(response: dict) -> str:\n",
    "    # Prefer the convenience field if present; otherwise collect text from parts\n",
    "    if \"output_text\" in response and response[\"output_text\"]:\n",
    "        return response[\"output_text\"]\n",
    "    parts = []\n",
    "    for item in response.get(\"output\", []):\n",
    "        for c in item.get(\"content\", []):\n",
    "            if isinstance(c, dict) and \"text\" in c:\n",
    "                parts.append(c[\"text\"])\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "print(extract_text(response_json))\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e34581-b7da-4a9e-93af-363c255ae83c",
   "metadata": {},
   "source": [
    "## OpenAI Python SDK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97a2f9fd-c430-4559-b5a0-a50d4829a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-mini-2025-04-14\n",
      "\n",
      "I am ChatGPT, based on the GPT-4 architecture developed by OpenAI. OpenAI is an AI research lab focused on ensuring that artificial general intelligence benefits all of humanity.\n",
      "\n",
      "**Why I was created:**  \n",
      "I was designed to assist with a wide range of natural language tasks, such as answering questions, generating text, helping with creative projects, providing explanations, and supporting learning. The goal is to make complex information accessible, improve productivity, and enable new forms of human-computer interaction.\n",
      "\n",
      "**Strengths:**  \n",
      "- I can generate coherent and contextually relevant text across many topics.  \n",
      "- I have a broad knowledge base up to my last training cutoff (June 2024).  \n",
      "- I am capable of nuanced understanding and can handle creative and technical prompts alike.  \n",
      "- I support image input capabilities, allowing me to analyze and discuss visual content.  \n",
      "- I can adapt my tone and style for different audiences and purposes.\n",
      "\n",
      "**Weaknesses:**  \n",
      "- I do not have real-time access to the internet or events beyond my knowledge cutoff.  \n",
      "- I may occasionally produce plausible but incorrect or nonsensical answers.  \n",
      "- I lack true understanding or consciousness and rely purely on patterns learned from data.  \n",
      "- My responses can be sensitive to input phrasing and sometimes miss subtle context or user intent.  \n",
      "- I have limitations in deeply specialized or highly technical domains where updated or proprietary knowledge is needed.\n",
      "\n",
      "**When to choose me over other models:**  \n",
      "- When you need a sophisticated, conversational AI that can handle a wide range of topics with contextual awareness.  \n",
      "- If you require both textual and image understanding in a single assistant.  \n",
      "- When the task benefits from generating creative, detailed, or well-structured responses.  \n",
      "- For educational or support tasks where clarity, explanation, and adaptable style are important.  \n",
      "- When you want an AI developed with a strong emphasis on safety, ethical guidelines, and broad usability.\n",
      "\n",
      "In contrast, specialized models tuned for specific industries or tasks, or systems with real-time data integration, might be preferable in contexts requiring the utmost domain expertise or up-to-the-minute information.\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from lib.ws_minify import ws_minify\n",
    "from lib.get_random_ollama_model import get_random_ollama_model\n",
    "import random\n",
    "\n",
    "# Initialize the SDK client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Compose a user prompt. `textwrap.dedent` removes the common leading indentation,\n",
    "# which keeps the code readable without sending leading spaces to the model.\n",
    "user_prompt = ws_minify(\"\"\"\n",
    "    State your model and the company or lab that created you.\n",
    "    Then discuss a little bit about why you were created and what are your strengths and weaknesses.\n",
    "    Finally discuss situations in which it would be preferable to choose you over other models.\n",
    "\"\"\")\n",
    "\n",
    "all_models = client.models.list()\n",
    "openai_models = [model.id for model in all_models.data]\n",
    "\n",
    "def is_chat_model(model_id: str) -> bool:\n",
    "    \"\"\"Heuristic filter: include chat-capable LLMs; exclude embeddings/moderation/audio.\"\"\"\n",
    "    s = model_id.lower()\n",
    "    if any(bad in s for bad in (\n",
    "        \"embedding\", \"embed\", \"moderation\", \"whisper\",\n",
    "        \"tts\", \"audio\", \"transcribe\", \"pro\", \"turbo\",\n",
    "        \"search\", \"preview\"\n",
    "    )):\n",
    "        return False\n",
    "    return s.startswith((\"gpt-\", \"gpt\", \"o\", \"chatgpt\"))\n",
    "\n",
    "chat_models = sorted([m for m in openai_models if is_chat_model(m)])\n",
    "current_model = random.choice(chat_models)\n",
    "\n",
    "def extract_text(response) -> str:\n",
    "    if getattr(response, \"output_text\", None):\n",
    "        return response.output_text\n",
    "    # fallback: collect pieces\n",
    "    parts = []\n",
    "    for item in response.output or []:\n",
    "        for c in getattr(item, \"content\", []):\n",
    "            if hasattr(c, \"text\"):\n",
    "                parts.append(c.text)\n",
    "    return \"\".join(parts)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=current_model,\n",
    "    input=user_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# Print which model we used for clarity.\n",
    "print(f\"{current_model}\\n\")\n",
    "\n",
    "# The generated text\n",
    "print(extract_text(response))\n",
    "\n",
    "# Simple visual separator for readability.\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72abd8-b165-4e19-964f-3d560b269cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
